# 2-3,自动微分机制


神经网络通常依赖反向传播求梯度来更新网络参数，求梯度过程通常是一件非常复杂而容易出错的事情。

而深度学习框架可以帮助我们自动地完成这种求梯度运算。

Tensorflow一般使用梯度磁带tf.GradientTape来记录正向运算过程，然后反播磁带自动得到梯度值。

这种利用tf.GradientTape求微分的方法叫做Tensorflow的自动微分机制。


### 一，利用梯度磁带求导数

```python
import tensorflow as tf
import numpy as np 

# f(x) = a*x**2 + b*x + c的导数

x = tf.Variable(0.0,name = "x",dtype = tf.float32)
a = tf.constant(1.0)
b = tf.constant(-2.0)
c = tf.constant(1.0)

with tf.GradientTape() as tape:
    y = a*tf.pow(x,2) + b*x + c
    
dy_dx = tape.gradient(y,x)
print(dy_dx)
```

```
tf.Tensor(-2.0, shape=(), dtype=float32)
```

```python
# 对常量张量也可以求导，需要增加watch

with tf.GradientTape() as tape:
    tape.watch([a,b,c])
    y = a*tf.pow(x,2) + b*x + c
    
dy_dx,dy_da,dy_db,dy_dc = tape.gradient(y,[x,a,b,c])
print(dy_da)
print(dy_db)
print(dy_dc)

```

```
tf.Tensor(0.0, shape=(), dtype=float32)
tf.Tensor(1.0, shape=(), dtype=float32)
```

```python

```

```python
# 可以求二阶导数
with tf.GradientTape() as tape2:
    with tf.GradientTape() as tape1:   
        y = a*tf.pow(x,2) + b*x + c
    dy_dx = tape1.gradient(y,x)   
dy2_dx2 = tape2.gradient(dy_dx,x)

print(dy2_dx2)
```

```
tf.Tensor(2.0, shape=(), dtype=float32)
```

```python

```

```python
# 可以在autograph中使用

@tf.function
def f(x):   
    a = tf.constant(1.0)
    b = tf.constant(-2.0)
    c = tf.constant(1.0)
    
    # 自变量转换成tf.float32
    x = tf.cast(x,tf.float32)
    with tf.GradientTape() as tape:
        tape.watch(x)
        y = a*tf.pow(x,2)+b*x+c
    dy_dx = tape.gradient(y,x) 
    
    return((dy_dx,y))

tf.print(f(tf.constant(0.0)))
tf.print(f(tf.constant(1.0)))
```

```
(-2, 1)
(0, 0)
```

```python

```

### 二，利用梯度磁带和优化器求最小值

```python
# 求f(x) = a*x**2 + b*x + c的最小值
# 使用optimizer.apply_gradients

import time
time1 = time.time()

x = tf.Variable(0.0,name = "x",dtype = tf.float32)
a = tf.constant(1.0)
b = tf.constant(-2.0)
c = tf.constant(1.0)

optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)
# 学习率如果是1，直接就左右摇摆了
# 迭代一万次和迭代1000次得到的结果竟然一样，精度为什么没提高？原因可能是后期学习率不够大，已经不足以使x大幅移动了，因为导数本身已经趋于0了，
# 此外可能还有计算机本身的精度问题
for _ in range(1000):
    with tf.GradientTape() as tape:
        y = a*tf.pow(x,2) + b*x + c
    dy_dx = tape.gradient(y,x)
#     tf.print("y =",y,"; x =",x, "; dy_dx =", dy_dx)
    # 每次更新其实是x减去导数乘学习率
    optimizer.apply_gradients(grads_and_vars=[(dy_dx,x)])
    
tf.print("y =",y,"; x =",x)
print(time.time() - time1)

# 在学习停滞时，增大学习率，精度可以继续提高，其实这有些类似于梯度消失问题
# 但学习率过大也可能导致左右摇摆，尝试0.1，0.5，1
# print(dir(optimizer))
optimizer.learning_rate = 0.1
# optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)
for _ in range(10):
    with tf.GradientTape() as tape:
        y = a*tf.pow(x,2) + b*x + c
    dy_dx = tape.gradient(y,x)
    tf.print("y =",y,"; x =",x, "; dy_dx =", dy_dx)
    # 每次更新其实是x减去导数乘学习率
    optimizer.apply_gradients(grads_and_vars=[(dy_dx,x)])
    
tf.print("y =",y,"; x =",x)
```

```
y = 0 ; x = 0.999998569
```

```python

```

```python
# 求f(x) = a*x**2 + b*x + c的最小值
# 使用optimizer.minimize
# optimizer.minimize相当于先用tape求gradient,再apply_gradient

x = tf.Variable(0.0,name = "x",dtype = tf.float32)

#注意f()无参数
def f():   
    a = tf.constant(1.0)
    b = tf.constant(-2.0)
    c = tf.constant(1.0)
    y = a*tf.pow(x,2)+b*x+c
    return(y)

optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)   
for _ in range(1000):
    optimizer.minimize(f,[x])
    
tf.print("y =",f(),"; x =",x)
```

```
y = 0 ; x = 0.999998569
```

```python

```

```python
# 在autograph中完成最小值求解
# 使用optimizer.apply_gradients
time1 = time.time()

x = tf.Variable(0.0,name = "x",dtype = tf.float32)
optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)

@tf.function
def minimizef():
    a = tf.constant(1.0)
    b = tf.constant(-2.0)
    c = tf.constant(1.0)
    
    for _ in tf.range(10000): #注意autograph时使用tf.range(1000)而不是range(1000)
        with tf.GradientTape() as tape:
            y = a*tf.pow(x,2) + b*x + c
        dy_dx = tape.gradient(y,x)
        optimizer.apply_gradients(grads_and_vars=[(dy_dx,x)])
        
    y = a*tf.pow(x,2) + b*x + c
    return y

tf.print(minimizef())
tf.print(x)
print(time.time() - time1)  # 确实比上面快了很多，实际速度应该更快，因为这里面可能还包含了编译所用的时间
# 忽略编译时间的话，确实快太多了，上面执行10000次循环需要7s多，这里从1000次到10000次，时间从0.2s变为0.3s，
# 说明每0.1s可以执行1000次左右，而上面执行1000次需要0.7s
```

```
0
0.999998569
```

```python

```

```python
# 在autograph中完成最小值求解
# 使用optimizer.minimize

time1 = time.time()

x = tf.Variable(0.0,name = "x",dtype = tf.float32)
optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)   

@tf.function
def f():   
    a = tf.constant(1.0)
    b = tf.constant(-2.0)
    c = tf.constant(1.0)
    y = a*tf.pow(x,2)+b*x+c
    return(y)

@tf.function
def train(epoch):  
    for _ in tf.range(epoch):  
        optimizer.minimize(f,[x])
    return(f())


tf.print(train(1000))
tf.print(x)
print(time.time() - time1)  # 一体化之后比上面略快一点点
```

```
0
0.999998569
```

```python

```

如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号"算法美食屋"下留言。作者时间和精力有限，会酌情予以回复。

也可以在公众号后台回复关键字：**加群**，加入读者交流群和大家讨论。

![算法美食屋二维码.jpg](./data/算法美食屋二维码.jpg)
